{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化实现\n",
    "\n",
    "本笔记本展示了向量化实现与非向量化实现的性能对比，以及向量化在梯度下降中的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 非向量化实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_vectorized_implementation(X, theta):\n",
    "    \"\"\"\n",
    "    非向量化实现的预测函数\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    theta: 参数向量 (n, 1)\n",
    "    \n",
    "    返回:\n",
    "    predictions: 预测值向量 (m, 1)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  # 样本数量\n",
    "    predictions = np.zeros((m, 1))  # 初始化预测值\n",
    "    \n",
    "    # 使用嵌套循环计算预测值\n",
    "    for i in range(m):\n",
    "        prediction = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            prediction += X[i, j] * theta[j, 0]\n",
    "        predictions[i, 0] = prediction\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 向量化实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_implementation(X, theta):\n",
    "    \"\"\"\n",
    "    向量化实现的预测函数\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    theta: 参数向量 (n, 1)\n",
    "    \n",
    "    返回:\n",
    "    predictions: 预测值向量 (m, 1)\n",
    "    \"\"\"\n",
    "    # 使用矩阵乘法计算预测值\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 性能对比测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance():\n",
    "    \"\"\"\n",
    "    比较向量化与非向量化实现的性能\n",
    "    \"\"\"\n",
    "    # 生成大规模数据\n",
    "    m = 10000  # 10000个样本\n",
    "    n = 100    # 100个特征\n",
    "    X = np.random.rand(m, n)\n",
    "    theta = np.random.rand(n, 1)\n",
    "    \n",
    "    # 测试非向量化实现\n",
    "    start_time = time.time()\n",
    "    non_vectorized_result = non_vectorized_implementation(X, theta)\n",
    "    non_vectorized_time = time.time() - start_time\n",
    "    \n",
    "    # 测试向量化实现\n",
    "    start_time = time.time()\n",
    "    vectorized_result = vectorized_implementation(X, theta)\n",
    "    vectorized_time = time.time() - start_time\n",
    "    \n",
    "    # 计算速度提升\n",
    "    speed_up = non_vectorized_time / vectorized_time\n",
    "    \n",
    "    # 验证结果一致性\n",
    "    max_difference = np.max(np.abs(non_vectorized_result - vectorized_result))\n",
    "    \n",
    "    print(f\"非向量化实现时间: {non_vectorized_time:.6f} 秒\")\n",
    "    print(f\"向量化实现时间: {vectorized_time:.6f} 秒\")\n",
    "    print(f\"向量化速度提升: {speed_up:.2f} 倍\")\n",
    "    print(f\"结果最大差异: {max_difference:.10f}\")\n",
    "\n",
    "# 运行性能对比\n",
    "compare_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 向量化梯度下降实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    向量化实现的梯度下降算法\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含偏置项\n",
    "    y: 目标值向量 (m, 1)\n",
    "    theta: 初始参数向量 (n+1, 1)\n",
    "    learning_rate: 学习率\n",
    "    n_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习后的参数向量\n",
    "    cost_history: 每次迭代的代价函数值\n",
    "    \"\"\"\n",
    "    m = len(y)  # 样本数量\n",
    "    cost_history = np.zeros(n_iterations)  # 记录代价函数历史\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # 向量化计算梯度\n",
    "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        # 更新参数\n",
    "        theta = theta - learning_rate * gradients\n",
    "        # 计算并记录代价函数值\n",
    "        cost_history[i] = (1/(2*m)) * np.sum(np.square(X.dot(theta) - y))\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 向量化梯度下降示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模拟数据\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "X = 2 * np.random.rand(n_samples, n_features)\n",
    "true_theta = np.array([[5], [10], [15], [20], [25]])\n",
    "y = X.dot(true_theta) + np.random.randn(n_samples, 1) * 0.1\n",
    "\n",
    "# 添加偏置项\n",
    "X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "\n",
    "# 初始化参数\n",
    "theta_initial = np.random.randn(n_features + 1, 1)\n",
    "\n",
    "# 设置超参数\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "\n",
    "# 使用向量化梯度下降训练模型\n",
    "theta, cost_history = vectorized_gradient_descent(X_b, y, theta_initial, learning_rate, n_iterations)\n",
    "\n",
    "# 打印结果\n",
    "print(\"真实参数:\")\n",
    "print(true_theta.ravel())\n",
    "\n",
    "print(\"\\n学习到的参数:\")\n",
    "print(theta.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(n_iterations), cost_history)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('代价函数值')\n",
    "plt.title('向量化梯度下降训练过程')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 向量化的优势总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"向量化实现的优势:\")\n",
    "print(\"1. 代码更简洁易读\")\n",
    "print(\"2. 执行速度显著提升（尤其是大规模数据）\")\n",
    "print(\"3. 减少了手动循环，降低了出错概率\")\n",
    "print(\"4. 充分利用了硬件加速（如GPU）\")\n",
    "print(\"5. 与数学公式的表达更加一致\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}