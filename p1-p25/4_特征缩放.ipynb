{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征缩放\n",
    "\n",
    "本笔记本实现了特征缩放的两种方法：标准化和归一化，并展示了特征缩放对梯度下降性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 生成不同尺度的特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_different_scales():\n",
    "    \"\"\"\n",
    "    生成具有不同尺度特征的模拟数据\n",
    "    \n",
    "    返回:\n",
    "    X: 特征数据 (n_samples, 2)，两个特征尺度差异很大\n",
    "    y: 目标值 (n_samples, 1)\n",
    "    \"\"\"\n",
    "    n_samples = 100\n",
    "    # 生成大范围特征 (0-100)\n",
    "    X1 = np.random.rand(n_samples, 1) * 100  \n",
    "    # 生成小范围特征 (0-1)\n",
    "    X2 = np.random.rand(n_samples, 1) * 1    \n",
    "    # 合并特征\n",
    "    X = np.hstack((X1, X2))\n",
    "    # 生成目标值\n",
    "    y = 2 * X1 + 3 * X2 + np.random.randn(n_samples, 1) * 5\n",
    "    return X, y\n",
    "\n",
    "# 生成数据\n",
    "X, y = generate_data_with_different_scales()\n",
    "print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "print(f\"特征1范围: {np.min(X[:, 0]):.2f} 到 {np.max(X[:, 0]):.2f}\")\n",
    "print(f\"特征2范围: {np.min(X[:, 1]):.2f} 到 {np.max(X[:, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 可视化原始特征分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X[:, 0], bins=20, alpha=0.6)\n",
    "plt.xlabel('特征1值')\n",
    "plt.ylabel('频率')\n",
    "plt.title('特征1分布 (大范围)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(X[:, 1], bins=20, alpha=0.6)\n",
    "plt.xlabel('特征2值')\n",
    "plt.ylabel('频率')\n",
    "plt.title('特征2分布 (小范围)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 标准化 (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X):\n",
    "    \"\"\"\n",
    "    对特征进行标准化处理\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    \n",
    "    返回:\n",
    "    X_normalized: 标准化后的特征矩阵\n",
    "    mean: 每个特征的均值\n",
    "    std: 每个特征的标准差\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std\n",
    "\n",
    "# 标准化特征\n",
    "X_normalized, mean, std = normalize_features(X)\n",
    "print(f\"标准化后特征1范围: {np.min(X_normalized[:, 0]):.2f} 到 {np.max(X_normalized[:, 0]):.2f}\")\n",
    "print(f\"标准化后特征2范围: {np.min(X_normalized[:, 1]):.2f} 到 {np.max(X_normalized[:, 1]):.2f}\")\n",
    "print(f\"特征均值: {mean}\")\n",
    "print(f\"特征标准差: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 归一化 (Min-Max Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(X):\n",
    "    \"\"\"\n",
    "    对特征进行归一化处理\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n)\n",
    "    \n",
    "    返回:\n",
    "    X_scaled: 归一化后的特征矩阵\n",
    "    min_val: 每个特征的最小值\n",
    "    max_val: 每个特征的最大值\n",
    "    \"\"\"\n",
    "    min_val = np.min(X, axis=0)\n",
    "    max_val = np.max(X, axis=0)\n",
    "    X_scaled = (X - min_val) / (max_val - min_val)\n",
    "    return X_scaled, min_val, max_val\n",
    "\n",
    "# 归一化特征\n",
    "X_minmax, min_val, max_val = min_max_scaling(X)\n",
    "print(f\"归一化后特征1范围: {np.min(X_minmax[:, 0]):.2f} 到 {np.max(X_minmax[:, 0]):.2f}\")\n",
    "print(f\"归一化后特征2范围: {np.min(X_minmax[:, 1]):.2f} 到 {np.max(X_minmax[:, 1]):.2f}\")\n",
    "print(f\"特征最小值: {min_val}\")\n",
    "print(f\"特征最大值: {max_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 可视化缩放后的特征分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "plt.xlabel('特征1')\n",
    "plt.ylabel('特征2')\n",
    "plt.title('原始特征分布')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_normalized[:, 0], X_normalized[:, 1], alpha=0.6)\n",
    "plt.xlabel('标准化特征1')\n",
    "plt.ylabel('标准化特征2')\n",
    "plt.title('标准化后特征分布')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_minmax[:, 0], X_minmax[:, 1], alpha=0.6)\n",
    "plt.xlabel('归一化特征1')\n",
    "plt.ylabel('归一化特征2')\n",
    "plt.title('归一化后特征分布')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 梯度下降实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    使用梯度下降算法训练线性回归模型\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含偏置项\n",
    "    y: 目标值向量 (m, 1)\n",
    "    theta: 初始参数向量 (n+1, 1)\n",
    "    learning_rate: 学习率\n",
    "    n_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习后的参数向量\n",
    "    cost_history: 每次迭代的代价函数值\n",
    "    \"\"\"\n",
    "    m = len(y)  # 样本数量\n",
    "    cost_history = np.zeros(n_iterations)  # 记录代价函数历史\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # 计算预测值\n",
    "        predictions = X.dot(theta)\n",
    "        # 计算误差\n",
    "        errors = predictions - y\n",
    "        # 计算梯度\n",
    "        gradients = (1/m) * X.T.dot(errors)\n",
    "        # 更新参数\n",
    "        theta = theta - learning_rate * gradients\n",
    "        # 记录当前代价函数值\n",
    "        cost_history[i] = (1/(2*m)) * np.sum(np.square(errors))\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 比较不同特征缩放方法对梯度下降的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为不同缩放方法的特征添加偏置项\n",
    "X_b_original = np.c_[np.ones((len(X), 1)), X]\n",
    "X_b_normalized = np.c_[np.ones((len(X_normalized), 1)), X_normalized]\n",
    "X_b_minmax = np.c_[np.ones((len(X_minmax), 1)), X_minmax]\n",
    "\n",
    "# 初始化参数\n",
    "theta_initial = np.random.randn(3, 1)\n",
    "n_iterations = 1000\n",
    "\n",
    "# 训练模型\n",
    "# 注意：原始特征需要更小的学习率\n",
    "theta_original, cost_history_original = gradient_descent(X_b_original, y, theta_initial, 0.000001, n_iterations)\n",
    "theta_normalized, cost_history_normalized = gradient_descent(X_b_normalized, y, theta_initial, 0.1, n_iterations)\n",
    "theta_minmax, cost_history_minmax = gradient_descent(X_b_minmax, y, theta_initial, 0.1, n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 可视化训练过程对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(n_iterations), cost_history_original, label='原始特征')\n",
    "plt.plot(range(n_iterations), cost_history_normalized, label='标准化特征')\n",
    "plt.plot(range(n_iterations), cost_history_minmax, label='归一化特征')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('代价函数值')\n",
    "plt.title('特征缩放对梯度下降的影响')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"梯度下降结果:\")\n",
    "print(f\"原始特征最终代价: {cost_history_original[-1]:.4f}\")\n",
    "print(f\"标准化特征最终代价: {cost_history_normalized[-1]:.4f}\")\n",
    "print(f\"归一化特征最终代价: {cost_history_minmax[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n学习到的参数:\")\n",
    "print(f\"原始特征: {theta_original.ravel()}\")\n",
    "print(f\"标准化特征: {theta_normalized.ravel()}\")\n",
    "print(f\"归一化特征: {theta_minmax.ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 特征缩放总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"特征缩放的优点:\")\n",
    "print(\"1. 加速梯度下降的收敛速度\")\n",
    "print(\"2. 允许使用更大的学习率\")\n",
    "print(\"3. 避免数值计算问题\")\n",
    "print(\"4. 使不同尺度的特征具有可比性\")\n",
    "\n",
    "print(\"\\n标准化 vs 归一化:\")\n",
    "print(\"- 标准化: 将特征转换为均值为0，标准差为1的分布\")\n",
    "print(\"- 归一化: 将特征缩放到[0, 1]或[-1, 1]的范围\")\n",
    "print(\"- 标准化更适合大多数机器学习算法\")\n",
    "print(\"- 归一化适合需要特征在特定范围的算法\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}