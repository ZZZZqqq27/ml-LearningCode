{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络与深度学习入门\n",
    "\n",
    "本笔记本介绍了神经网络的基本结构、前向传播、使用NumPy实现推理，以及TensorFlow的基本使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 神经网络基本结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"神经网络基本结构:\")\n",
    "print(\"1. 输入层：接收原始特征数据\")\n",
    "print(\"2. 隐藏层：处理和转换特征\")\n",
    "print(\"3. 输出层：产生最终预测结果\")\n",
    "\n",
    "print(\"\\n神经网络的数学表示:\")\n",
    "print(\"- 每个神经元：z = w·x + b，a = σ(z)\")\n",
    "print(\"- w：权重参数\")\n",
    "print(\"- b：偏置参数\")\n",
    "print(\"- σ：激活函数\")\n",
    "print(\"- a：神经元输出\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid激活函数\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU激活函数\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    tanh激活函数\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "# 可视化激活函数\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('Sigmoid激活函数')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z, relu(z))\n",
    "plt.title('ReLU激活函数')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z, tanh(z))\n",
    "plt.title('tanh激活函数')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 前向传播实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    初始化神经网络参数\n",
    "    \n",
    "    参数:\n",
    "    layer_dims: 包含各层神经元数量的列表\n",
    "    \n",
    "    返回:\n",
    "    parameters: 包含权重和偏置的字典\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # 设置随机种子以保证结果可重复\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        # 初始化权重，使用He初始化\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2 / layer_dims[l-1])\n",
    "        # 初始化偏置为0\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    神经网络前向传播\n",
    "    \n",
    "    参数:\n",
    "    X: 输入特征 (n_features, m_samples)\n",
    "    parameters: 包含权重和偏置的字典\n",
    "    \n",
    "    返回:\n",
    "    AL: 输出层激活值\n",
    "    caches: 包含中间值的字典，用于反向传播\n",
    "    \"\"\"\n",
    "    caches = {}\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # 网络层数\n",
    "    \n",
    "    # 前向传播通过隐藏层\n",
    "    for l in range(1, L):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        \n",
    "        # 线性变换\n",
    "        Z = np.dot(W, A) + b\n",
    "        # 应用ReLU激活函数\n",
    "        A = relu(Z)\n",
    "        \n",
    "        # 保存中间值\n",
    "        caches['Z' + str(l)] = Z\n",
    "        caches['A' + str(l)] = A\n",
    "    \n",
    "    # 输出层\n",
    "    WL = parameters['W' + str(L)]\n",
    "    bL = parameters['b' + str(L)]\n",
    "    ZL = np.dot(WL, A) + bL\n",
    "    AL = sigmoid(ZL)  # 二分类问题使用sigmoid\n",
    "    \n",
    "    caches['Z' + str(L)] = ZL\n",
    "    caches['A' + str(L)] = AL\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 神经网络推理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构\n",
    "layer_dims = [2, 4, 4, 1]  # 2个输入特征，2个隐藏层（各4个神经元），1个输出\n",
    "\n",
    "# 初始化参数\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "# 生成示例输入\n",
    "X = np.array([[0.5, 0.2], [0.3, 0.8]]).T  # 2个样本，每个样本2个特征\n",
    "print(f\"输入形状: {X.shape}\")\n",
    "\n",
    "# 前向传播（推理）\n",
    "AL, caches = forward_propagation(X, parameters)\n",
    "\n",
    "print(f\"输出预测: {AL.ravel()}\")\n",
    "print(f\"预测类别: {(AL > 0.5).astype(int).ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用NumPy实现简单神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    简单的三层神经网络实现\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        初始化神经网络\n",
    "        \n",
    "        参数:\n",
    "        input_dim: 输入特征维度\n",
    "        hidden_dim: 隐藏层神经元数量\n",
    "        output_dim: 输出层神经元数量\n",
    "        \"\"\"\n",
    "        # 初始化参数\n",
    "        self.W1 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.b1 = np.zeros((hidden_dim, 1))\n",
    "        self.W2 = np.random.randn(output_dim, hidden_dim) * np.sqrt(2 / hidden_dim)\n",
    "        self.b2 = np.zeros((output_dim, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        X: 输入特征 (n_features, m_samples)\n",
    "        \n",
    "        返回:\n",
    "        y_pred: 预测结果\n",
    "        \"\"\"\n",
    "        # 隐藏层\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        \n",
    "        # 输出层\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        y_pred = sigmoid(Z2)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        预测类别\n",
    "        \n",
    "        参数:\n",
    "        X: 输入特征 (n_features, m_samples)\n",
    "        threshold: 阈值\n",
    "        \n",
    "        返回:\n",
    "        predictions: 预测类别\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > threshold).astype(int)\n",
    "\n",
    "# 创建神经网络实例\n",
    "nn = SimpleNeuralNetwork(input_dim=2, hidden_dim=4, output_dim=1)\n",
    "\n",
    "# 测试预测\n",
    "X_test = np.array([[0.1, 0.9], [0.8, 0.2], [0.5, 0.5]]).T\n",
    "predictions = nn.predict(X_test)\n",
    "print(f\"预测类别: {predictions.ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorFlow 基础介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow版本: {tf.__version__}\")\n",
    "    \n",
    "    # 创建常量张量\n",
    "    a = tf.constant(5.0)\n",
    "    b = tf.constant(3.0)\n",
    "    \n",
    "    # 执行计算\n",
    "    c = a * b\n",
    "    print(f\"TensorFlow计算: 5.0 * 3.0 = {c.numpy()}\")\n",
    "    \n",
    "    # 创建变量\n",
    "    W = tf.Variable(tf.random.normal(shape=(2, 3)))\n",
    "    b = tf.Variable(tf.zeros(shape=(3,)))\n",
    "    print(f\"权重形状: {W.shape}\")\n",
    "    print(f\"偏置形状: {b.shape}\")\n",
    "    \n",
    "    # 使用tf.data创建数据集\n",
    "    X = tf.random.normal(shape=(100, 2))\n",
    "    y = tf.random.uniform(shape=(100, 1), minval=0, maxval=2, dtype=tf.int32)\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.batch(32).shuffle(100)\n",
    "    \n",
    "    print(\"\\nTensorFlow数据集示例:\")\n",
    "    for batch_X, batch_y in dataset.take(1):\n",
    "        print(f\"批处理X形状: {batch_X.shape}\")\n",
    "        print(f\"批处理y形状: {batch_y.shape}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"TensorFlow未安装，跳过TensorFlow示例\")\n",
    "    print(\"请使用 'pip install tensorflow' 安装TensorFlow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 深度学习入门总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"神经网络基本概念:\")\n",
    "print(\"1. 神经网络是由多个神经元组成的层级结构\")\n",
    "print(\"2. 每个神经元执行线性变换后接非线性激活函数\")\n",
    "print(\"3. 前向传播是从输入到输出的计算过程\")\n",
    "print(\"4. 反向传播用于更新参数\")\n",
    "\n",
    "print(\"\\n常用激活函数:\")\n",
    "print(\"1. Sigmoid: 用于二分类输出层\")\n",
    "print(\"2. ReLU: 用于隐藏层，解决梯度消失问题\")\n",
    "print(\"3. Tanh: 用于隐藏层，输出范围在[-1,1]\")\n",
    "print(\"4. Softmax: 用于多分类输出层\")\n",
    "\n",
    "print(\"\\n深度学习框架:\")\n",
    "print(\"1. TensorFlow: Google开发，功能强大\")\n",
    "print(\"2. PyTorch: Facebook开发，动态计算图\")\n",
    "print(\"3. Keras: 高级API，可基于TensorFlow或其他后端\")\n",
    "\n",
    "print(\"\\n深度学习应用:\")\n",
    "print(\"1. 计算机视觉: 图像分类、目标检测、图像分割\")\n",
    "print(\"2. 自然语言处理: 文本分类、机器翻译、情感分析\")\n",
    "print(\"3. 语音识别: 语音转文本、语音助手\")\n",
    "print(\"4. 推荐系统: 商品推荐、内容推荐\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}