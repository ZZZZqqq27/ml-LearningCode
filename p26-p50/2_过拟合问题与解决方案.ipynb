{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合问题与解决方案\n",
    "\n",
    "本笔记本展示了过拟合问题的识别和解决方法，包括正则化技术在线性回归和逻辑回归中的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 生成过拟合示例数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overfitting_data():\n",
    "    \"\"\"\n",
    "    生成用于演示过拟合问题的数据\n",
    "    \n",
    "    返回:\n",
    "    X: 特征数据 (n_samples, 1)\n",
    "    y: 目标值 (n_samples, 1)\n",
    "    \"\"\"\n",
    "    n_samples = 20\n",
    "    # 生成特征值\n",
    "    X = 6 * np.random.rand(n_samples, 1) - 3\n",
    "    # 生成目标值，添加较多噪声\n",
    "    y = 0.5 * X**3 + X**2 + X + 2 + np.random.randn(n_samples, 1) * 5\n",
    "    return X, y\n",
    "\n",
    "# 生成数据\n",
    "X, y = generate_overfitting_data()\n",
    "print(f\"数据形状: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 可视化原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.xlabel('特征 X')\n",
    "plt.ylabel('目标值 y')\n",
    "plt.title('过拟合示例数据')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 多项式回归与过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    创建多项式特征\n",
    "    \n",
    "    参数:\n",
    "    X: 原始特征矩阵 (m, 1)\n",
    "    degree: 多项式次数\n",
    "    \n",
    "    返回:\n",
    "    X_poly: 多项式特征矩阵 (m, degree+1)\n",
    "    \"\"\"\n",
    "    X_poly = np.ones((len(X), 1))\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, np.power(X, d)))\n",
    "    return X_poly\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    使用正规方程求解线性回归\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 目标值向量 (m, 1)\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习到的参数向量 (n+1, 1)\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    使用学习到的参数进行预测\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    theta: 参数向量 (n+1, 1)\n",
    "    \n",
    "    返回:\n",
    "    predictions: 预测值向量 (m, 1)\n",
    "    \"\"\"\n",
    "    return X.dot(theta)\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算均方误差\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实目标值 (m, 1)\n",
    "    y_pred: 预测值 (m, 1)\n",
    "    \n",
    "    返回:\n",
    "    mse: 均方误差值\n",
    "    \"\"\"\n",
    "    return np.mean(np.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 不同阶数多项式的拟合效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 15]\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    # 创建多项式特征\n",
    "    X_poly = create_polynomial_features(X, degree)\n",
    "    # 训练模型\n",
    "    theta = linear_regression(X_poly, y)\n",
    "    \n",
    "    # 生成测试数据用于绘制曲线\n",
    "    X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "    X_new_poly = create_polynomial_features(X_new, degree)\n",
    "    y_pred = predict(X_new_poly, theta)\n",
    "    \n",
    "    # 计算训练集上的MSE\n",
    "    y_train_pred = predict(X_poly, theta)\n",
    "    mse = compute_mse(y, y_train_pred)\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X, y, alpha=0.6, label='训练数据')\n",
    "    plt.plot(X_new, y_pred, 'r-', linewidth=2, label=f'{degree}阶多项式')\n",
    "    plt.xlabel('特征 X')\n",
    "    plt.ylabel('目标值 y')\n",
    "    plt.title(f'{degree}阶多项式 (MSE: {mse:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 正则化线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, alpha):\n",
    "    \"\"\"\n",
    "    岭回归（正则化线性回归）\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 目标值向量 (m, 1)\n",
    "    alpha: 正则化参数\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习到的参数向量 (n+1, 1)\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    # 岭回归的正规方程解\n",
    "    theta = np.linalg.inv(X.T.dot(X) + alpha * np.eye(n_features)).dot(X.T).dot(y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 不同正则化参数的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 15  # 高次多项式\n",
    "alphas = [0, 1, 100]\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    # 创建多项式特征\n",
    "    X_poly = create_polynomial_features(X, degree)\n",
    "    # 训练岭回归模型\n",
    "    theta = ridge_regression(X_poly, y, alpha)\n",
    "    \n",
    "    # 生成测试数据用于绘制曲线\n",
    "    X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "    X_new_poly = create_polynomial_features(X_new, degree)\n",
    "    y_pred = predict(X_new_poly, theta)\n",
    "    \n",
    "    # 计算训练集上的MSE\n",
    "    y_train_pred = predict(X_poly, theta)\n",
    "    mse = compute_mse(y, y_train_pred)\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X, y, alpha=0.6, label='训练数据')\n",
    "    plt.plot(X_new, y_pred, 'r-', linewidth=2, label=f'岭回归 (alpha={alpha})')\n",
    "    plt.xlabel('特征 X')\n",
    "    plt.ylabel('目标值 y')\n",
    "    plt.title(f'岭回归 (alpha={alpha}, MSE: {mse:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 正则化逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid函数\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost_logistic_regularized(X, y, theta, alpha):\n",
    "    \"\"\"\n",
    "    计算正则化逻辑回归的成本函数\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 标签向量 (m, 1)\n",
    "    theta: 参数向量 (n+1, 1)\n",
    "    alpha: 正则化参数\n",
    "    \n",
    "    返回:\n",
    "    cost: 成本函数值\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # 计算成本函数（包含正则化项）\n",
    "    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + \n",
    "           (alpha/(2*m)) * np.sum(np.square(theta[1:]))  # 注意：不正则化偏置项\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def gradient_descent_logistic_regularized(X, y, theta, learning_rate, alpha, n_iterations):\n",
    "    \"\"\"\n",
    "    使用梯度下降算法训练正则化逻辑回归模型\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)\n",
    "    y: 标签向量 (m, 1)\n",
    "    theta: 初始参数向量 (n+1, 1)\n",
    "    learning_rate: 学习率\n",
    "    alpha: 正则化参数\n",
    "    n_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习后的参数向量\n",
    "    cost_history: 每次迭代的成本函数值\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # 计算预测值\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        # 计算梯度（包含正则化项）\n",
    "        gradient = (1/m) * X.T.dot(h - y)\n",
    "        # 对非偏置项添加正则化梯度\n",
    "        gradient[1:] += (alpha/m) * theta[1:]\n",
    "        # 更新参数\n",
    "        theta = theta - learning_rate * gradient\n",
    "        # 记录成本函数值\n",
    "        cost_history[i] = compute_cost_logistic_regularized(X, y, theta, alpha)\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 正则化解决过拟合总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"过拟合问题总结:\")\n",
    "print(\"1. 过拟合表现：模型在训练数据上表现很好，但在测试数据上表现较差\")\n",
    "print(\"2. 过拟合原因：模型复杂度太高，学习了训练数据中的噪声\")\n",
    "print(\"3. 解决方法：\")\n",
    "print(\"   - 增加训练数据\")\n",
    "print(\"   - 减少特征数量\")\n",
    "print(\"   - 使用正则化技术\")\n",
    "print(\"   - 使用交叉验证选择模型\")\n",
    "\n",
    "print(\"\\n正则化技术:\")\n",
    "print(\"1. 岭回归（Ridge Regression）：L2正则化，添加参数平方和的惩罚项\")\n",
    "print(\"2. LASSO回归：L1正则化，添加参数绝对值和的惩罚项，可用于特征选择\")\n",
    "print(\"3. 弹性网络（Elastic Net）：结合L1和L2正则化\")\n",
    "\n",
    "print(\"\\n正则化参数alpha:\")\n",
    "print(\"- alpha=0: 无正则化，可能过拟合\")\n",
    "print(\"- alpha过大: 正则化太强，可能欠拟合\")\n",
    "print(\"- 需要通过交叉验证选择合适的alpha值\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}