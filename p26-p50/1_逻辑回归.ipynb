{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "\n",
    "本笔记本实现了逻辑回归模型，包括决策边界可视化、成本函数和梯度下降的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 生成二分类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_classification_data():\n",
    "    \"\"\"\n",
    "    生成二分类数据\n",
    "    \n",
    "    返回:\n",
    "    X: 特征数据 (n_samples, 2)\n",
    "    y: 标签 (n_samples, 1)，取值为0或1\n",
    "    \"\"\"\n",
    "    n_samples = 100\n",
    "    \n",
    "    # 生成第一类数据\n",
    "    X1 = np.random.randn(n_samples//2, 2) + np.array([2, 2])\n",
    "    y1 = np.zeros((n_samples//2, 1))\n",
    "    \n",
    "    # 生成第二类数据\n",
    "    X2 = np.random.randn(n_samples//2, 2) + np.array([-2, -2])\n",
    "    y2 = np.ones((n_samples//2, 1))\n",
    "    \n",
    "    # 合并数据\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.vstack((y1, y2))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成数据\n",
    "X, y = generate_binary_classification_data()\n",
    "print(f\"数据形状: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 可视化原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y[:, 0] == 0][:, 0], X[y[:, 0] == 0][:, 1], alpha=0.6, label='类别 0')\n",
    "plt.scatter(X[y[:, 0] == 1][:, 0], X[y[:, 0] == 1][:, 1], alpha=0.6, label='类别 1')\n",
    "plt.xlabel('特征 1')\n",
    "plt.ylabel('特征 2')\n",
    "plt.title('二分类数据')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 实现sigmoid函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid函数实现\n",
    "    \n",
    "    参数:\n",
    "    z: 输入值\n",
    "    \n",
    "    返回:\n",
    "    sigmoid(z): sigmoid函数值\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 可视化sigmoid函数\n",
    "z = np.linspace(-10, 10, 100)\n",
    "s = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, s)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid函数')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 实现逻辑回归的成本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, theta):\n",
    "    \"\"\"\n",
    "    计算逻辑回归的成本函数\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含偏置项\n",
    "    y: 标签向量 (m, 1)\n",
    "    theta: 参数向量 (n+1, 1)\n",
    "    \n",
    "    返回:\n",
    "    cost: 成本函数值\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # 计算成本函数\n",
    "    cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实现逻辑回归的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(X, y, theta, learning_rate, n_iterations):\n",
    "    \"\"\"\n",
    "    使用梯度下降算法训练逻辑回归模型\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含偏置项\n",
    "    y: 标签向量 (m, 1)\n",
    "    theta: 初始参数向量 (n+1, 1)\n",
    "    learning_rate: 学习率\n",
    "    n_iterations: 迭代次数\n",
    "    \n",
    "    返回:\n",
    "    theta: 学习后的参数向量\n",
    "    cost_history: 每次迭代的成本函数值\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # 计算预测值\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        # 计算梯度\n",
    "        gradient = (1/m) * X.T.dot(h - y)\n",
    "        # 更新参数\n",
    "        theta = theta - learning_rate * gradient\n",
    "        # 记录成本函数值\n",
    "        cost_history[i] = compute_cost_logistic(X, y, theta)\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加偏置项\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "# 初始化参数\n",
    "theta_initial = np.zeros((3, 1))\n",
    "\n",
    "# 设置超参数\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "\n",
    "# 训练模型\n",
    "theta, cost_history = gradient_descent_logistic(X_b, y, theta_initial, learning_rate, n_iterations)\n",
    "\n",
    "# 打印学习结果\n",
    "print(f\"学习到的参数: theta0 = {theta[0][0]:.4f}, theta1 = {theta[1][0]:.4f}, theta2 = {theta[2][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(n_iterations), cost_history)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('成本函数值')\n",
    "plt.title('逻辑回归的梯度下降训练过程')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 可视化决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, theta):\n",
    "    \"\"\"\n",
    "    绘制逻辑回归的决策边界\n",
    "    \n",
    "    参数:\n",
    "    X: 特征数据 (m, 2)\n",
    "    y: 标签 (m, 1)\n",
    "    theta: 学习到的参数 (3, 1)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # 绘制数据点\n",
    "    plt.scatter(X[y[:, 0] == 0][:, 0], X[y[:, 0] == 0][:, 1], alpha=0.6, label='类别 0')\n",
    "    plt.scatter(X[y[:, 0] == 1][:, 0], X[y[:, 0] == 1][:, 1], alpha=0.6, label='类别 1')\n",
    "    \n",
    "    # 绘制决策边界 (sigmoid(z) = 0.5 时，z = 0)\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                           np.linspace(x2_min, x2_max, 100))\n",
    "    \n",
    "    # 生成网格点并预测\n",
    "    grid = np.c_[np.ones((100*100, 1)), xx1.ravel(), xx2.ravel()]\n",
    "    probs = sigmoid(grid.dot(theta)).reshape(xx1.shape)\n",
    "    \n",
    "    # 绘制决策边界 (概率为0.5的等高线)\n",
    "    plt.contour(xx1, xx2, probs, levels=[0.5], linewidths=2, colors='red')\n",
    "    \n",
    "    plt.xlabel('特征 1')\n",
    "    plt.ylabel('特征 2')\n",
    "    plt.title('逻辑回归的决策边界')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 绘制决策边界\n",
    "plot_decision_boundary(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型预测与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"\n",
    "    使用逻辑回归模型进行预测\n",
    "    \n",
    "    参数:\n",
    "    X: 特征矩阵 (m, n+1)，包含偏置项\n",
    "    theta: 学习到的参数\n",
    "    threshold: 阈值，默认为0.5\n",
    "    \n",
    "    返回:\n",
    "    predictions: 预测结果 (m, 1)\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(X.dot(theta))\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# 预测训练数据\n",
    "predictions = predict(X_b, theta)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"模型准确率: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}