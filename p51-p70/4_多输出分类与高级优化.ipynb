{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输出分类与高级优化\n",
    "\n",
    "本笔记本介绍了多类分类和多标签分类的区别，以及高级优化算法（如Adam）的实现和应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 多类分类 vs 多标签分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"多类分类与多标签分类的区别:\")\n",
    "\n",
    "print(\"\\n多类分类 (Multi-class Classification):\")\n",
    "print(\"1. 定义: 每个样本属于且仅属于多个类别中的一个\")\n",
    "print(\"2. 示例: \")\n",
    "print(\"   - 图像分类 (猫、狗、鸟)\")\n",
    "print(\"   - 手写数字识别 (0-9)\")\n",
    "print(\"   - 情感分析 (积极、中性、消极)\")\n",
    "print(\"3. 输出层: \")\n",
    "print(\"   - 神经元数量 = 类别数量\")\n",
    "print(\"   - 激活函数: Softmax\")\n",
    "print(\"   - 输出: 概率分布，和为1\")\n",
    "print(\"4. 损失函数: 多类交叉熵\")\n",
    "\n",
    "print(\"\\n多标签分类 (Multi-label Classification):\")\n",
    "print(\"1. 定义: 每个样本可以同时属于多个类别\")\n",
    "print(\"2. 示例: \")\n",
    "print(\"   - 图像标注 (一张图片可以同时包含猫和狗)\")\n",
    "print(\"   - 文本分类 (一篇文章可以同时属于科技和教育类别)\")\n",
    "print(\"   - 音乐类型分类 (一首歌可以同时属于摇滚和流行)\")\n",
    "print(\"3. 输出层: \")\n",
    "print(\"   - 神经元数量 = 标签数量\")\n",
    "print(\"   - 激活函数: Sigmoid (每个标签独立判断)\")\n",
    "print(\"   - 输出: 每个标签的独立概率\")\n",
    "print(\"4. 损失函数: 二元交叉熵（每个标签独立计算）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 多类分类的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多类分类数据生成\n",
    "def generate_multiclass_data(n_samples=200, n_features=2, n_classes=3):\n",
    "    \"\"\"\n",
    "    生成多类分类数据\n",
    "    \n",
    "    参数:\n",
    "    n_samples: 样本数量\n",
    "    n_features: 特征维度\n",
    "    n_classes: 类别数量\n",
    "    \n",
    "    返回:\n",
    "    X: 特征矩阵 (n_samples, n_features)\n",
    "    y: 标签向量 (n_samples,)\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import make_classification\n",
    "    \n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_features,\n",
    "        n_redundant=0,\n",
    "        n_classes=n_classes,\n",
    "        n_clusters_per_class=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成并可视化数据\n",
    "X, y = generate_multiclass_data(n_samples=200, n_features=2, n_classes=3)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(3):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], label=f'类别 {i}')\n",
    "plt.title('多类分类数据集')\n",
    "plt.xlabel('特征 1')\n",
    "plt.ylabel('特征 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "print(f\"类别: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 多标签分类的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多标签分类数据生成\n",
    "def generate_multilabel_data(n_samples=200, n_features=2, n_labels=3, n_classes_per_sample=2):\n",
    "    \"\"\"\n",
    "    生成多标签分类数据\n",
    "    \n",
    "    参数:\n",
    "    n_samples: 样本数量\n",
    "    n_features: 特征维度\n",
    "    n_labels: 标签数量\n",
    "    n_classes_per_sample: 每个样本平均的标签数量\n",
    "    \n",
    "    返回:\n",
    "    X: 特征矩阵 (n_samples, n_features)\n",
    "    y: 标签矩阵 (n_samples, n_labels)\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import make_multilabel_classification\n",
    "    \n",
    "    X, y = make_multilabel_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_classes=n_labels,\n",
    "        n_labels=n_classes_per_sample,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 生成并可视化数据\n",
    "X_ml, y_ml = generate_multilabel_data(n_samples=200, n_features=2, n_labels=3)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 原始数据分布\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_ml[:, 0], X_ml[:, 1])\n",
    "plt.title('多标签分类数据集')\n",
    "plt.xlabel('特征 1')\n",
    "plt.ylabel('特征 2')\n",
    "plt.grid(True)\n",
    "\n",
    "# 标签1的分布\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_ml[y_ml[:, 0] == 1, 0], X_ml[y_ml[:, 0] == 1, 1], c='r', label='标签 1')\n",
    "plt.scatter(X_ml[y_ml[:, 0] == 0, 0], X_ml[y_ml[:, 0] == 0, 1], c='gray', alpha=0.3)\n",
    "plt.title('标签 1 的分布')\n",
    "plt.xlabel('特征 1')\n",
    "plt.ylabel('特征 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 标签2的分布\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_ml[y_ml[:, 1] == 1, 0], X_ml[y_ml[:, 1] == 1, 1], c='g', label='标签 2')\n",
    "plt.scatter(X_ml[y_ml[:, 1] == 0, 0], X_ml[y_ml[:, 1] == 0, 1], c='gray', alpha=0.3)\n",
    "plt.title('标签 2 的分布')\n",
    "plt.xlabel('特征 1')\n",
    "plt.ylabel('特征 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"数据形状: X={X_ml.shape}, y={y_ml.shape}\")\n",
    "print(f\"标签示例:\")\n",
    "print(y_ml[:5])\n",
    "print(f\"每个样本的标签数量:\")\n",
    "print(np.sum(y_ml, axis=1)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 高级优化算法：Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adam优化算法:\")\n",
    "print(\"1. 定义: Adam (Adaptive Moment Estimation) 是一种自适应学习率优化算法\")\n",
    "print(\"2. 特点: \")\n",
    "print(\"   - 结合了AdaGrad和RMSProp的优点\")\n",
    "print(\"   - 计算每个参数的自适应学习率\")\n",
    "print(\"   - 利用动量加速收敛\")\n",
    "print(\"   - 对噪声和稀疏梯度具有鲁棒性\")\n",
    "print(\"3. 超参数:\")\n",
    "print(\"   - learning_rate: 学习率 (默认: 0.001)\")\n",
    "print(\"   - beta1: 一阶动量的指数衰减率 (默认: 0.9)\")\n",
    "print(\"   - beta2: 二阶动量的指数衰减率 (默认: 0.999)\")\n",
    "print(\"   - epsilon: 防止除零的小常数 (默认: 1e-7)\")\n",
    "print(\"4. 优势:\")\n",
    "print(\"   - 不需要手动调整学习率\")\n",
    "print(\"   - 收敛速度快\")\n",
    "print(\"   - 适用于大规模数据和参数\")\n",
    "print(\"   - 适用于非平稳目标和噪声问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adam优化算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Adam优化器实现\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        初始化Adam优化器\n",
    "        \n",
    "        参数:\n",
    "        learning_rate: 学习率\n",
    "        beta1: 一阶动量的指数衰减率\n",
    "        beta2: 二阶动量的指数衰减率\n",
    "        epsilon: 防止除零的小常数\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # 一阶动量\n",
    "        self.v = None  # 二阶动量\n",
    "        self.t = 0     # 时间步\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        \"\"\"\n",
    "        使用Adam算法更新参数\n",
    "        \n",
    "        参数:\n",
    "        params: 当前参数\n",
    "        gradients: 参数的梯度\n",
    "        \n",
    "        返回:\n",
    "        更新后的参数\n",
    "        \"\"\"\n",
    "        # 初始化动量\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        # 时间步增加\n",
    "        self.t += 1\n",
    "        \n",
    "        # 计算一阶动量（带偏置校正）\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        \n",
    "        # 计算二阶动量（带偏置校正）\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # 更新参数\n",
    "        params -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adam优化器的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试Adam优化器\n",
    "def test_function(x):\n",
    "    \"\"\"\n",
    "    测试函数: f(x) = x^2\n",
    "    最小值在x=0处\n",
    "    \"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "def test_function_gradient(x):\n",
    "    \"\"\"\n",
    "    测试函数的梯度: f'(x) = 2x\n",
    "    \"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "# 初始化\n",
    "x = 5.0  # 初始值\n",
    "optimizer = AdamOptimizer(learning_rate=0.1)\n",
    "steps = 20\n",
    "history = []\n",
    "\n",
    "# 优化过程\n",
    "for i in range(steps):\n",
    "    value = test_function(x)\n",
    "    history.append(value)\n",
    "    print(f\"Step {i+1}: x = {x:.6f}, f(x) = {value:.6f}\")\n",
    "    \n",
    "    # 计算梯度\n",
    "    gradient = test_function_gradient(x)\n",
    "    \n",
    "    # 更新参数\n",
    "    x = optimizer.update(x, gradient)\n",
    "\n",
    "# 可视化优化过程\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, steps+1), history)\n",
    "plt.title('Adam优化器的收敛过程')\n",
    "plt.xlabel('迭代步数')\n",
    "plt.ylabel('函数值')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 额外的层类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"神经网络中常见的层类型:\")\n",
    "\n",
    "print(\"\\n1. 全连接层 (Fully Connected Layer):\")\n",
    "print(\"   - 每个神经元与前一层的所有神经元相连\")\n",
    "print(\"   - 参数数量: input_size * output_size + output_size\")\n",
    "print(\"   - 用途: 特征提取和转换\")\n",
    "\n",
    "print(\"\\n2. 卷积层 (Convolutional Layer):\")\n",
    "print(\"   - 局部连接，权值共享\")\n",
    "print(\"   - 适合处理网格数据（图像）\")\n",
    "print(\"   - 保留空间结构信息\")\n",
    "\n",
    "print(\"\\n3. 池化层 (Pooling Layer):\")\n",
    "print(\"   - 下采样，减少特征维度\")\n",
    "print(\"   - 最大池化和平均池化\")\n",
    "print(\"   - 增加模型的鲁棒性\")\n",
    "\n",
    "print(\"\\n4. 批归一化层 (Batch Normalization Layer):\")\n",
    "print(\"   - 对每批数据进行归一化\")\n",
    "print(\"   - 加速训练，减少内部协变量偏移\")\n",
    "print(\"   - 可以使用更高的学习率\")\n",
    "\n",
    "print(\"\\n5.  dropout层 (Dropout Layer):\")\n",
    "print(\"   - 随机失活神经元\")\n",
    "print(\"   - 防止过拟合\")\n",
    "print(\"   - 模拟集成学习\")\n",
    "\n",
    "print(\"\\n6. 激活层 (Activation Layer):\")\n",
    "print(\"   - 应用非线性激活函数\")\n",
    "print(\"   - ReLU、Sigmoid、tanh等\")\n",
    "print(\"   - 引入非线性能力\")\n",
    "\n",
    "print(\"\\n7. 循环层 (Recurrent Layer):\")\n",
    "print(\"   - 处理序列数据\")\n",
    "print(\"   - 包含记忆单元\")\n",
    "print(\"   - LSTM、GRU等变体\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"计算图的基本概念:\")\n",
    "print(\"1. 定义: 计算图是一种表示数学计算的有向图\")\n",
    "print(\"2. 组成: \")\n",
    "print(\"   - 节点 (Nodes): 表示操作或变量\")\n",
    "print(\"   - 边 (Edges): 表示数据流向\")\n",
    "print(\"3. 优势: \")\n",
    "print(\"   - 自动微分: 便于梯度计算\")\n",
    "print(\"   - 并行计算: 识别可并行的操作\")\n",
    "print(\"   - 内存优化: 高效管理计算资源\")\n",
    "print(\"   - 代码可读性: 清晰表示计算流程\")\n",
    "\n",
    "print(\"\\n深度学习框架中的计算图:\")\n",
    "print(\"- TensorFlow: 静态计算图\")\n",
    "print(\"- PyTorch: 动态计算图\")\n",
    "print(\"- JAX: 函数式编程风格的计算图\")\n",
    "\n",
    "print(\"\\n计算图的应用:\")\n",
    "print(\"1. 前向传播: 从输入到输出的计算\")\n",
    "print(\"2. 反向传播: 从损失到参数的梯度计算\")\n",
    "print(\"3. 模型优化: 基于梯度的参数更新\")\n",
    "print(\"4. 模型部署: 导出为可执行格式\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 更大的神经网络示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个更大的神经网络，包含批归一化和dropout\n",
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    深层神经网络实现\n",
    "    包含批归一化和dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        初始化深层神经网络\n",
    "        \n",
    "        参数:\n",
    "        input_size: 输入特征维度\n",
    "        hidden_sizes: 隐藏层大小列表\n",
    "        output_size: 输出维度\n",
    "        dropout_rate: dropout比率\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # 初始化参数\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.batch_norm_params = []\n",
    "        \n",
    "        # 输入层到第一个隐藏层\n",
    "        self.weights.append(np.random.randn(input_size, hidden_sizes[0]) * np.sqrt(2 / input_size))\n",
    "        self.biases.append(np.zeros(hidden_sizes[0]))\n",
    "        self.batch_norm_params.append({\n",
    "            'gamma': np.ones(hidden_sizes[0]),\n",
    "            'beta': np.zeros(hidden_sizes[0])\n",
    "        })\n",
    "        \n",
    "        # 隐藏层之间\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.weights.append(np.random.randn(hidden_sizes[i-1], hidden_sizes[i]) * np.sqrt(2 / hidden_sizes[i-1]))\n",
    "            self.biases.append(np.zeros(hidden_sizes[i]))\n",
    "            self.batch_norm_params.append({\n",
    "                'gamma': np.ones(hidden_sizes[i]),\n",
    "                'beta': np.zeros(hidden_sizes[i])\n",
    "            })\n",
    "        \n",
    "        # 最后一个隐藏层到输出层\n",
    "        self.weights.append(np.random.randn(hidden_sizes[-1], output_size) * np.sqrt(2 / hidden_sizes[-1]))\n",
    "        self.biases.append(np.zeros(output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"\n",
    "        ReLU激活函数\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax激活函数\n",
    "        \"\"\"\n",
    "        max_x = np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def batch_norm(self, x, gamma, beta, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        批归一化\n",
    "        \"\"\"\n",
    "        mean = np.mean(x, axis=0, keepdims=True)\n",
    "        var = np.var(x, axis=0, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(var + epsilon)\n",
    "        return gamma * x_norm + beta\n",
    "    \n",
    "    def dropout(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Dropout层\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            mask = np.random.binomial(1, 1 - self.dropout_rate, size=x.shape)\n",
    "            return x * mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        \n",
    "        # 隐藏层\n",
    "        for i in range(len(self.hidden_sizes)):\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            Z_norm = self.batch_norm(Z, self.batch_norm_params[i]['gamma'], self.batch_norm_params[i]['beta'])\n",
    "            A = self.relu(Z_norm)\n",
    "            A = self.dropout(A, training)\n",
    "        \n",
    "        # 输出层\n",
    "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
    "        A = self.softmax(Z)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测类别\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X, training=False)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# 测试深层神经网络\n",
    "print(\"测试深层神经网络:\")\n",
    "\n",
    "# 创建示例数据\n",
    "batch_size = 10\n",
    "input_size = 10\n",
    "hidden_sizes = [64, 32, 16]  # 三个隐藏层\n",
    "output_size = 3  # 3个类别\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "\n",
    "# 初始化网络\n",
    "model = DeepNeuralNetwork(input_size, hidden_sizes, output_size, dropout_rate=0.3)\n",
    "\n",
    "# 前向传播\n",
    "probabilities = model.forward(X, training=True)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(f\"输入形状: {X.shape}\")\n",
    "print(f\"概率输出形状: {probabilities.shape}\")\n",
    "print(f\"预测结果形状: {predictions.shape}\")\n",
    "\n",
    "print(\"\\n概率输出:\")\n",
    "print(probabilities)\n",
    "\n",
    "print(\"\\n每一行的和:\")\n",
    "print(np.sum(probabilities, axis=1))\n",
    "\n",
    "print(\"\\n预测结果:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"多输出分类与高级优化总结:\")\n",
    "\n",
    "print(\"\\n1. 多类分类与多标签分类的区别:\")\n",
    "print(\"   - 多类分类: 每个样本仅属于一个类别，使用Softmax激活\")\n",
    "print(\"   - 多标签分类: 每个样本可属于多个类别，使用Sigmoid激活\")\n",
    "\n",
    "print(\"\\n2. 高级优化算法:\")\n",
    "print(\"   - Adam: 结合动量和自适应学习率\")\n",
    "print(\"   - 优势: 收敛快，不需要手动调整学习率\")\n",
    "print(\"   - 超参数: 学习率、beta1、beta2、epsilon\")\n",
    "\n",
    "print(\"\\n3. 额外的层类型:\")\n",
    "print(\"   - 全连接层: 基本的特征转换\")\n",
    "print(\"   - 卷积层: 处理网格数据\")\n",
    "print(\"   - 池化层: 下采样\")\n",
    "print(\"   - 批归一化层: 加速训练\")\n",
    "print(\"   - Dropout层: 防止过拟合\")\n",
    "\n",
    "print(\"\\n4. 计算图:\")\n",
    "print(\"   - 表示计算流程的有向图\")\n",
    "print(\"   - 便于自动微分和并行计算\")\n",
    "print(\"   - 深度学习框架的核心概念\")\n",
    "\n",
    "print(\"\\n5. 更大的神经网络:\")\n",
    "print(\"   - 多层结构: 捕获更复杂的特征\")\n",
    "print(\"   - 批归一化: 加速深层网络训练\")\n",
    "print(\"   - Dropout: 提高模型泛化能力\")\n",
    "print(\"   - 适当的初始化: 避免梯度消失/爆炸\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}