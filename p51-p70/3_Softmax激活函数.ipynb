{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax激活函数：原理与实现\n",
    "\n",
    "本笔记本介绍了Softmax激活函数的原理、实现以及在多类别分类中的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax激活函数的基本原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Softmax激活函数的基本原理:\")\n",
    "print(\"1. 定义: Softmax函数将一个K维实值向量转换为一个K维概率分布\")\n",
    "print(\"2. 公式: Softmax(x_i) = e^{x_i} / Σ_{j=1 to K} e^{x_j}\")\n",
    "print(\"3. 特性: \")\n",
    "print(\"   - 输出值在(0, 1)之间\")\n",
    "print(\"   - 所有输出值的和为1\")\n",
    "print(\"   - 对输入值的相对大小敏感\")\n",
    "print(\"   - 不改变输入值的相对顺序\")\n",
    "\n",
    "print(\"\\n应用场景:\")\n",
    "print(\"- 多类别分类问题的输出层\")\n",
    "print(\"- 需要概率分布输出的场景\")\n",
    "print(\"- 强化学习中的策略网络\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax函数的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax激活函数\n",
    "    \n",
    "    参数:\n",
    "    x: 输入向量或矩阵 (batch_size, num_classes)\n",
    "    \n",
    "    返回:\n",
    "    归一化的概率分布\n",
    "    \"\"\"\n",
    "    # 处理批次输入\n",
    "    if x.ndim == 1:\n",
    "        # 单个样本\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    else:\n",
    "        # 批量样本\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# 测试Softmax函数\n",
    "print(\"单个样本测试:\")\n",
    "x_single = np.array([2.0, 1.0, 0.1])\n",
    "softmax_single = softmax(x_single)\n",
    "print(f\"输入: {x_single}\")\n",
    "print(f\"Softmax输出: {softmax_single}\")\n",
    "print(f\"输出和: {np.sum(softmax_single)}\")\n",
    "\n",
    "print(\"\\n批量样本测试:\")\n",
    "x_batch = np.array([[2.0, 1.0, 0.1],\n",
    "                    [0.5, 1.5, 2.5],\n",
    "                    [-1.0, 0.0, 1.0]])\n",
    "softmax_batch = softmax(x_batch)\n",
    "print(f\"输入形状: {x_batch.shape}\")\n",
    "print(f\"Softmax输出:\")\n",
    "print(softmax_batch)\n",
    "print(f\"每一行的和: {np.sum(softmax_batch, axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Softmax函数的数值稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"数值稳定性问题:\")\n",
    "print(\"- 当输入值很大时，指数运算可能导致数值溢出\")\n",
    "print(\"- 当输入值很小时，指数运算可能导致数值下溢\")\n",
    "\n",
    "print(\"\\n解决方案:\")\n",
    "print(\"- 减去输入向量中的最大值，使输入值的范围更合理\")\n",
    "print(\"- 这样可以避免指数运算的数值问题\")\n",
    "print(\"- 数学上等价于原始Softmax函数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数值稳定的Softmax实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    \"\"\"\n",
    "    数值稳定的Softmax激活函数\n",
    "    通过减去最大值来避免数值溢出\n",
    "    \n",
    "    参数:\n",
    "    x: 输入向量或矩阵 (batch_size, num_classes)\n",
    "    \n",
    "    返回:\n",
    "    归一化的概率分布\n",
    "    \"\"\"\n",
    "    # 处理批次输入\n",
    "    if x.ndim == 1:\n",
    "        # 单个样本\n",
    "        max_x = np.max(x)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    else:\n",
    "        # 批量样本\n",
    "        max_x = np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x - max_x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# 测试数值稳定性\n",
    "print(\"测试数值稳定性:\")\n",
    "x_large = np.array([1000.0, 999.0, 998.0])\n",
    "\n",
    "print(\"\\n使用普通Softmax:\")\n",
    "try:\n",
    "    result_normal = softmax(x_large)\n",
    "    print(f\"结果: {result_normal}\")\n",
    "except Exception as e:\n",
    "    print(f\"错误: {e}\")\n",
    "\n",
    "print(\"\\n使用稳定Softmax:\")\n",
    "try:\n",
    "    result_stable = stable_softmax(x_large)\n",
    "    print(f\"结果: {result_stable}\")\n",
    "    print(f\"和: {np.sum(result_stable)}\")\n",
    "except Exception as e:\n",
    "    print(f\"错误: {e}\")\n",
    "\n",
    "# 验证两种实现的结果是否一致\n",
    "print(\"\\n验证两种实现的一致性:\")\n",
    "x_test = np.array([[2.0, 1.0, 0.1], [0.5, 1.5, 2.5]])\n",
    "result_normal = softmax(x_test)\n",
    "result_stable = stable_softmax(x_test)\n",
    "print(f\"普通Softmax结果:\")\n",
    "print(result_normal)\n",
    "print(f\"稳定Softmax结果:\")\n",
    "print(result_stable)\n",
    "print(f\"结果是否一致: {np.allclose(result_normal, result_stable)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Softmax在神经网络中的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例: 带有Softmax输出的神经网络\n",
    "class NeuralNetworkWithSoftmax:\n",
    "    \"\"\"\n",
    "    带有Softmax输出层的简单神经网络\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        初始化神经网络参数\n",
    "        \n",
    "        参数:\n",
    "        input_size: 输入特征维度\n",
    "        hidden_size: 隐藏层神经元数量\n",
    "        output_size: 输出类别数量\n",
    "        \"\"\"\n",
    "        # 初始化权重\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        X: 输入特征 (batch_size, input_size)\n",
    "        \n",
    "        返回:\n",
    "        模型的概率输出 (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # 隐藏层\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = np.maximum(0, Z1)  # ReLU激活\n",
    "        \n",
    "        # 输出层\n",
    "        Z2 = np.dot(A1, self.W2) + self.b2\n",
    "        A2 = stable_softmax(Z2)  # Softmax激活\n",
    "        \n",
    "        return A2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测类别\n",
    "        \n",
    "        参数:\n",
    "        X: 输入特征 (batch_size, input_size)\n",
    "        \n",
    "        返回:\n",
    "        预测的类别索引 (batch_size,)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# 测试神经网络\n",
    "print(\"测试带有Softmax输出的神经网络:\")\n",
    "\n",
    "# 创建示例数据\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "hidden_size = 16\n",
    "output_size = 3  # 3个类别\n",
    "\n",
    "X = np.random.randn(batch_size, input_size)\n",
    "\n",
    "# 初始化网络\n",
    "model = NeuralNetworkWithSoftmax(input_size, hidden_size, output_size)\n",
    "\n",
    "# 前向传播\n",
    "probabilities = model.forward(X)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(f\"输入形状: {X.shape}\")\n",
    "print(f\"概率输出形状: {probabilities.shape}\")\n",
    "print(f\"预测结果形状: {predictions.shape}\")\n",
    "\n",
    "print(\"\\n概率输出:\")\n",
    "print(probabilities)\n",
    "\n",
    "print(\"\\n每一行的和:\")\n",
    "print(np.sum(probabilities, axis=1))\n",
    "\n",
    "print(\"\\n预测结果:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Softmax的改进实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Softmax的改进实现:\")\n",
    "print(\"1. 数值稳定性改进:\")\n",
    "print(\"   - 减去最大值避免数值溢出\")\n",
    "print(\"   - 适用于批量输入\")\n",
    "\n",
    "print(\"\\n2. 计算效率改进:\")\n",
    "print(\"   - 向量化实现，避免循环\")\n",
    "print(\"   - 利用NumPy的广播功能\")\n",
    "print(\"   - 对于大规模计算，考虑使用GPU加速\")\n",
    "\n",
    "print(\"\\n3. 与损失函数的联合优化:\")\n",
    "print(\"   - Softmax + 交叉熵损失的联合计算\")\n",
    "print(\"   - 数值稳定性更好\")\n",
    "print(\"   - 计算效率更高\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Softmax与交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Softmax与交叉熵损失函数的联合计算\n",
    "    \n",
    "    参数:\n",
    "    y_pred: 模型的原始输出 (batch_size, num_classes)\n",
    "    y_true: 真实标签的one-hot编码 (batch_size, num_classes)\n",
    "    \n",
    "    返回:\n",
    "    平均交叉熵损失\n",
    "    \"\"\"\n",
    "    # 数值稳定的计算\n",
    "    batch_size = y_pred.shape[0]\n",
    "    \n",
    "    # 计算softmax\n",
    "    max_y = np.max(y_pred, axis=1, keepdims=True)\n",
    "    exp_y = np.exp(y_pred - max_y)\n",
    "    softmax_y = exp_y / np.sum(exp_y, axis=1, keepdims=True)\n",
    "    \n",
    "    # 计算交叉熵损失\n",
    "    loss = -np.sum(y_true * np.log(softmax_y + 1e-15)) / batch_size\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 测试损失函数\n",
    "print(\"测试Softmax交叉熵损失函数:\")\n",
    "\n",
    "y_pred = np.array([[2.0, 1.0, 0.1], [0.5, 1.5, 2.5]])\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])  # one-hot编码\n",
    "\n",
    "loss = softmax_cross_entropy_loss(y_pred, y_true)\n",
    "print(f\"预测输出:\")\n",
    "print(y_pred)\n",
    "print(f\"真实标签:\")\n",
    "print(y_true)\n",
    "print(f\"交叉熵损失: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Softmax的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_gradient(x):\n",
    "    \"\"\"\n",
    "    计算Softmax函数的梯度\n",
    "    \n",
    "    参数:\n",
    "    x: 输入向量 (num_classes,)\n",
    "    \n",
    "    返回:\n",
    "    梯度矩阵 (num_classes, num_classes)\n",
    "    \"\"\"\n",
    "    s = stable_softmax(x).reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "# 测试梯度计算\n",
    "print(\"测试Softmax梯度计算:\")\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "gradient = softmax_gradient(x)\n",
    "\n",
    "print(f\"输入: {x}\")\n",
    "print(f\"Softmax输出: {stable_softmax(x)}\")\n",
    "print(f\"梯度矩阵:\")\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Softmax总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Softmax激活函数总结:\")\n",
    "\n",
    "print(\"\\n1. 核心特性:\")\n",
    "print(\"   - 将实值向量转换为概率分布\")\n",
    "print(\"   - 输出值和为1\")\n",
    "print(\"   - 对输入的相对大小敏感\")\n",
    "\n",
    "print(\"\\n2. 实现要点:\")\n",
    "print(\"   - 数值稳定性: 减去最大值\")\n",
    "print(\"   - 批量处理: 支持批次输入\")\n",
    "print(\"   - 向量化: 利用NumPy的广播功能\")\n",
    "\n",
    "print(\"\\n3. 应用场景:\")\n",
    "print(\"   - 多类别分类的输出层\")\n",
    "print(\"   - 需要概率输出的模型\")\n",
    "print(\"   - 与交叉熵损失函数配合使用\")\n",
    "\n",
    "print(\"\\n4. 注意事项:\")\n",
    "print(\"   - 计算时注意数值稳定性\")\n",
    "print(\"   - 与交叉熵损失联合计算提高效率\")\n",
    "print(\"   - 在深层网络中，可能需要考虑梯度消失问题\")\n",
    "\n",
    "print(\"\\n5. 替代方案:\")\n",
    "print(\"   - 对于二分类: Sigmoid函数\")\n",
    "print(\"   - 对于多标签分类: 多个Sigmoid单元\")\n",
    "print(\"   - 对于某些场景: 直接使用线性输出\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}