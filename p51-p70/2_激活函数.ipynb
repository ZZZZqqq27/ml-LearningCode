{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数：原理与实现\n",
    "\n",
    "本笔记本介绍了不同类型的激活函数、它们的特性以及在神经网络中的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 为什么需要激活函数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"激活函数的重要性:\")\n",
    "print(\"1. 引入非线性: 没有激活函数的神经网络只是线性回归的堆叠，无法学习复杂的非线性模式\")\n",
    "print(\"2. 特征转换: 将输入特征转换为更适合后续层处理的表示形式\")\n",
    "print(\"3. 梯度流动: 良好的激活函数有助于梯度在反向传播中顺畅流动\")\n",
    "print(\"4. 输出范围控制: 某些激活函数可以将输出限制在特定范围内\")\n",
    "\n",
    "print(\"\\n如果没有激活函数:\")\n",
    "print(\"- 多层神经网络退化为线性模型\")\n",
    "print(\"- 无法学习复杂的函数关系\")\n",
    "print(\"- 网络深度失去意义\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid激活函数\n",
    "    公式: f(x) = 1 / (1 + e^(-x))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Sigmoid激活函数的导数\n",
    "    公式: f'(x) = f(x) * (1 - f(x))\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# 绘制Sigmoid函数及其导数\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_derivative = sigmoid_derivative(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid')\n",
    "plt.title('Sigmoid激活函数')\n",
    "plt.xlabel('输入值')\n",
    "plt.ylabel('输出值')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_derivative, label='Sigmoid导数', color='orange')\n",
    "plt.title('Sigmoid激活函数的导数')\n",
    "plt.xlabel('输入值')\n",
    "plt.ylabel('导数值')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid函数的特点:\")\n",
    "print(\"- 输出范围: (0, 1)\")\n",
    "print(\"- 平滑可导\")\n",
    "print(\"- 适合二分类问题的输出层\")\n",
    "print(\"- 缺点: 存在梯度消失问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid的替代方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 tanh激活函数\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    tanh激活函数\n",
    "    公式: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    tanh激活函数的导数\n",
    "    公式: f'(x) = 1 - f(x)^2\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# 3.2 ReLU激活函数\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU激活函数\n",
    "    公式: f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    ReLU激活函数的导数\n",
    "    公式: f'(x) = 1 if x > 0 else 0\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# 3.3 Leaky ReLU激活函数\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU激活函数\n",
    "    公式: f(x) = max(alpha*x, x)\n",
    "    \"\"\"\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU激活函数的导数\n",
    "    公式: f'(x) = 1 if x > 0 else alpha\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "# 3.4 ELU激活函数\n",
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ELU激活函数\n",
    "    公式: f(x) = x if x > 0 else alpha*(e^x - 1)\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    \"\"\"\n",
    "    ELU激活函数的导数\n",
    "    公式: f'(x) = 1 if x > 0 else alpha*e^x\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, 1, alpha * np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 激活函数的可视化比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有激活函数的值\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_tanh = tanh(x)\n",
    "y_relu = relu(x)\n",
    "y_leaky_relu = leaky_relu(x)\n",
    "y_elu = elu(x)\n",
    "\n",
    "# 绘制激活函数\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, y_sigmoid, label='Sigmoid', linewidth=2)\n",
    "plt.plot(x, y_tanh, label='tanh', linewidth=2)\n",
    "plt.plot(x, y_relu, label='ReLU', linewidth=2)\n",
    "plt.plot(x, y_leaky_relu, label='Leaky ReLU', linewidth=2)\n",
    "plt.plot(x, y_elu, label='ELU', linewidth=2)\n",
    "plt.title('各种激活函数的比较')\n",
    "plt.xlabel('输入值')\n",
    "plt.ylabel('输出值')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# 绘制激活函数的导数\n",
    "y_sigmoid_deriv = sigmoid_derivative(x)\n",
    "y_tanh_deriv = tanh_derivative(x)\n",
    "y_relu_deriv = relu_derivative(x)\n",
    "y_leaky_relu_deriv = leaky_relu_derivative(x)\n",
    "y_elu_deriv = elu_derivative(x)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x, y_sigmoid_deriv, label='Sigmoid导数', linewidth=2)\n",
    "plt.plot(x, y_tanh_deriv, label='tanh导数', linewidth=2)\n",
    "plt.plot(x, y_relu_deriv, label='ReLU导数', linewidth=2)\n",
    "plt.plot(x, y_leaky_relu_deriv, label='Leaky ReLU导数', linewidth=2)\n",
    "plt.plot(x, y_elu_deriv, label='ELU导数', linewidth=2)\n",
    "plt.title('各种激活函数导数的比较')\n",
    "plt.xlabel('输入值')\n",
    "plt.ylabel('导数值')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 如何选择激活函数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"激活函数选择指南:\")\n",
    "\n",
    "print(\"\\n1. 隐藏层激活函数:\")\n",
    "print(\"   - ReLU: 最常用，计算高效，缓解梯度消失问题\")\n",
    "print(\"   - Leaky ReLU: 解决ReLU的死亡神经元问题\")\n",
    "print(\"   - ELU: 具有ReLU的优点，同时在负数区域平滑\")\n",
    "print(\"   - tanh: 输出中心化，适合某些需要对称输出的场景\")\n",
    "\n",
    "print(\"\\n2. 输出层激活函数:\")\n",
    "print(\"   - 二分类问题: Sigmoid (输出范围0-1)\")\n",
    "print(\"   - 多分类问题: Softmax (输出概率分布)\")\n",
    "print(\"   - 回归问题: 线性激活函数 (无激活)\")\n",
    "print(\"   - 有界回归: Sigmoid或tanh\")\n",
    "\n",
    "print(\"\\n3. 特殊场景:\")\n",
    "print(\"   - RNN: tanh或LSTM的内部激活函数\")\n",
    "print(\"   - GAN: Leaky ReLU用于生成器，Sigmoid用于判别器\")\n",
    "print(\"   - 强化学习: tanh或ReLU\")\n",
    "\n",
    "print(\"\\n4. 性能考虑:\")\n",
    "print(\"   - ReLU及其变体: 计算速度快，适合深层网络\")\n",
    "print(\"   - Sigmoid/tanh: 计算较慢，可能导致梯度消失\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 多类别分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"多类别分类问题:\")\n",
    "print(\"1. 问题定义: 样本属于多个类别中的一个，互斥\")\n",
    "print(\"2. 输出层设计: 类别数为C时，输出层有C个神经元\")\n",
    "print(\"3. 激活函数: Softmax (将输出转换为概率分布)\")\n",
    "print(\"4. 损失函数: 交叉熵损失\")\n",
    "\n",
    "print(\"\\n多类别分类 vs 二分类:\")\n",
    "print(\"- 二分类: 一个输出神经元，Sigmoid激活\")\n",
    "print(\"- 多类别: C个输出神经元，Softmax激活\")\n",
    "\n",
    "print(\"\\n多类别分类 vs 多标签分类:\")\n",
    "print(\"- 多类别: 每个样本只属于一个类别\")\n",
    "print(\"- 多标签: 每个样本可以属于多个类别\")\n",
    "print(\"- 多标签分类通常使用Sigmoid激活函数而不是Softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 激活函数的实际应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例: 不同激活函数在神经网络中的应用\n",
    "def neural_network_layer(X, W, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    神经网络层的前向传播\n",
    "    \n",
    "    参数:\n",
    "    X: 输入特征 (batch_size, input_features)\n",
    "    W: 权重矩阵 (input_features, output_features)\n",
    "    b: 偏置向量 (output_features,)\n",
    "    activation: 激活函数名称 ('relu', 'sigmoid', 'tanh', 'leaky_relu', 'elu')\n",
    "    \n",
    "    返回:\n",
    "    Z: 线性变换结果\n",
    "    A: 激活函数输出\n",
    "    \"\"\"\n",
    "    # 线性变换\n",
    "    Z = np.dot(X, W) + b\n",
    "    \n",
    "    # 应用激活函数\n",
    "    if activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A = tanh(Z)\n",
    "    elif activation == 'leaky_relu':\n",
    "        A = leaky_relu(Z)\n",
    "    elif activation == 'elu':\n",
    "        A = elu(Z)\n",
    "    else:\n",
    "        raise ValueError(f\"未知的激活函数: {activation}\")\n",
    "    \n",
    "    return Z, A\n",
    "\n",
    "# 测试不同激活函数\n",
    "batch_size = 16\n",
    "input_features = 10\n",
    "output_features = 5\n",
    "\n",
    "X = np.random.randn(batch_size, input_features)\n",
    "W = np.random.randn(input_features, output_features) * np.sqrt(2 / input_features)\n",
    "b = np.zeros(output_features)\n",
    "\n",
    "print(\"输入形状:\", X.shape)\n",
    "\n",
    "for activation in ['relu', 'sigmoid', 'tanh', 'leaky_relu', 'elu']:\n",
    "    Z, A = neural_network_layer(X, W, b, activation=activation)\n",
    "    print(f\"\\n{activation}激活函数:\")\n",
    "    print(f\"  线性输出范围: [{Z.min():.4f}, {Z.max():.4f}]\")\n",
    "    print(f\"  激活输出范围: [{A.min():.4f}, {A.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 激活函数总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"激活函数总结:\")\n",
    "\n",
    "print(\"\\n1. 主要激活函数及其特性:\")\n",
    "print(\"   - Sigmoid: 输出范围0-1，适合二分类输出层\")\n",
    "print(\"   - tanh: 输出范围-1-1，输出中心化\")\n",
    "print(\"   - ReLU: 计算高效，缓解梯度消失，适合隐藏层\")\n",
    "print(\"   - Leaky ReLU: 解决ReLU的死亡神经元问题\")\n",
    "print(\"   - ELU: 平滑的负值区域，具有ReLU的优点\")\n",
    "\n",
    "print(\"\\n2. 选择建议:\")\n",
    "print(\"   - 隐藏层: 优先使用ReLU及其变体\")\n",
    "print(\"   - 输出层: 根据任务类型选择合适的激活函数\")\n",
    "print(\"   - 考虑计算效率和梯度流动特性\")\n",
    "\n",
    "print(\"\\n3. 最佳实践:\")\n",
    "print(\"   - 从ReLU开始尝试\")\n",
    "print(\"   - 如果遇到训练问题，尝试Leaky ReLU或ELU\")\n",
    "print(\"   - 为不同的层选择合适的激活函数\")\n",
    "print(\"   - 考虑批归一化对激活函数性能的影响\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}